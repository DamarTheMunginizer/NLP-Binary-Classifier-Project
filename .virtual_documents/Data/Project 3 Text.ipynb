








#Imports
import praw
import os
#Needed to read in the varibles that are sensitive.
from dotenv import load_dotenv
load_dotenv()

import numpy as np
import pandas as pd


#Initialize PRAW and Authenticate API
reddit = praw.Reddit(
    client_id = os.getenv('client_id'),
    client_secret = os.getenv('client_secret'),
    user_agent = 'text_classifier:v1.0 (by DvisionaryS)',
    username = 'DvisionaryS',
    password = os.getenv('password')
)


# Assign a variable to the subreddits of your choosing
subreddit_1 = reddit.subreddit('dogs')
subreddit_2 = reddit.subreddit('personalfinance')


# Function to find post for the dataset where n: represents the number of post we are trying to pull in.
def fetch_posts(subreddit, n=1000):
    posts = []
    for post in subreddit.new(limit=n):
        posts.append({
            'subreddit': post.subreddit,
            'title': post.title,
            'selftext': post.selftext,
            'utc': post.created_utc
            
        })
    return pd.DataFrame(posts)





#Read in the completed data
df = pd.read_csv('./p3_data/final_reddit_data.csv')





#View how much data we have stored
df.shape


df['subreddit'].value_counts()


#View the data
df.head()


#Check for duplicates
df['utc'].value_counts()


#Check for null values
df.isnull().sum()


#Fill in the null values
df['selftext'] = df['selftext'].fillna('')


df.isnull().sum()





#Preprocessing Read-Ins
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics


df['text'] = (df['title'] + ' : ' + df['selftext']).apply(preprocess_text)


X = df['text']
y = df['subreddit']


# Check what's needed to check in a classification problem.
y.value_counts(normalize=True)





X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


vect = CountVectorizer(ngram_range=(1, 3), stop_words='english')


vect.fit(X_train)


X_train_vec = vect.transform(X_train)
X_test_vec = vect.transform(X_test)


vect.get_feature_names_out()[2000:2020]


X_train_df = pd.DataFrame(
    X_train_vec.toarray(),
    columns=vect.get_feature_names_out()
)


X_train_df.head()


X_train_df.sum().sort_values(ascending=False).head(10).plot(kind='barh');


# Review the TTS X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['text'])  # Convert text to numerical data
y = df['subreddit']


#baseline score
y_test.value_counts(normalize=True)


most_freq = y_train.mode()[0]
baseline = (y_train == most_freq).mean()
print(f"Baseline Accuracy: {baseline:.2f}")


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)


lr_model_pred = lr_model.predict(X_test)
lr_model_pred;


accuracy = accuracy_score(y_test, lr_model_pred)
print(f"Model Accuracy: {accuracy:.2f}")


lr_model.score(X_train, y_train)


lr_model.score(X_test, y_test)


df['predicted_subreddit'] = df['text'].apply(lambda x: lr_model.predict(vectorizer.transform([x]))[0])


df.head()





new_data = [
    "What breed is best for a small apartment?",
    "What are the top 5 ways to start for retirement?",
    "My pup loves playing fetch",
    "How can I invest in index funds?"
]


def predict_subreddit(new_posts):
    # Preprocess the new posts
    new_posts_processed = [preprocess_text(post) for post in new_posts]
    # Vectorize the preprocessed text
    new_posts_vec = vectorizer.transform(new_posts_processed)
    # Predict the subreddits using the model
    predictions = lr_model.predict(new_posts_vec)
    return predictions


df_new = pd.DataFrame(new_data, columns=["text"])


predictions = predict_subreddit(new_data)
for post, prediction in zip(new_data, predictions):
    print(f"Post: {post}\nPredicted subreddit: {prediction}\n")


# Step 3: Add the predictions as a new column
df_new["predicted_subreddit"] = predictions

# Step 4: Display the resulting DataFrame
df_new.head()


stop_words = set(stopwords.words('english'))  # Default stop words
df['processed_text'] = df['text']


new_post = "mans best friend"


newes_t = "What bank should I use?"


#Encode target
encoder = LabelEncoder()
y = encoder.fit_transform(df['subreddit'])


#Build the model; Logistic Regression and Random Forest
# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
lr = LogisticRegression()
lr.fit(X_train, y_train)

# Evaluate model
lr_pred = lr.predict(X_test)

lr_accuracy = accuracy_score(y_test, lr_preds)


print("Logistic Regression:\n", classification_report(y_test, y_pred))


lr_accuracy


rf = RandomForestClassifier()  
rf.fit(X_train, y_train)  
rf_preds = rf.predict(X_test)

rf_accuracy = accuracy_score(y_test, rf_preds)


print("Random Forest:\n", classification_report(y_test, y_pred_rf))


rf_accuracy


conf_matrix_lr = confusion_matrix(y_test, y_pred)


conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)


fig, ax = plt.subplots(1, 2, figsize=(12, 6))

sns.heatmap(conf_matrix_lr, annot=True, fmt='d', cmap='Blues', ax=ax[0])
ax[0].set_title('Logistic Regression Confusion Matrix')

sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues', ax=ax[1])
ax[1].set_title('Random Forest Confusion Matrix')

plt.show()


baseline_accuracy = y_test.value_counts(normalize=True)


# Print model comparison
print("Logistic Regression vs Random Forest Comparison:")
print(f"Logistic Regression - Accuracy: {lr_accuracy}")
print(f"Random Forest - Accuracy: {rf_accuracy}")




