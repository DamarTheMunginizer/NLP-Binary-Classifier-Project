import praw
import os
import pandas as pd
import requests


import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from nltk.stem import WordNetLemmatizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.ensemble import RandomForestClassifier

import matplotlib.pyplot as plt
import seaborn as sns


df = pd.read_csv('./reddit_combined_df.csv')


def preprocess_text(text):
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))

    words = text.split()
    processed_words = [
    lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    
    processed_text = " ".join(processed_words)
    return processed_text


df['selftext'] = df['selftext'].fillna('')


df['text'] = (df['title'] + ' : ' + df['selftext']).apply(preprocess_text)


df.head()


df['subreddit'].value_counts()


X = df['text']
y = df['subreddit']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


vect = CountVectorizer(ngram_range=(1, 3), stop_words='english', max_features=5000)
X_train_vec = vect.fit_transform(X_train)
X_test_vec = vect.transform(X_test)


lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train_vec, y_train)


lr_pred = lr_model.predict(X_test_vec)
print(classification_report(y_test, lr_pred))


lr_model.score(X_train_vec, y_train)


lr_model.score(X_test_vec, y_test)


df['predicted_subreddit'] = df['text'].apply(lambda x: lr_model.predict(vect.transform([x]))[0])


df['correct'] = df['predicted_subreddit'] == df['subreddit']


df.head(2)


new_data = [
    "What breed is best for a small apartment?",
    "What are the top 5 ways to start for retirement?",
    "My pup loves playing fetch",
    "How can I invest in index funds?"
]


def predict_subreddit(new_posts):
    # Preprocess the new posts
    new_posts_processed = [preprocess_text(post) for post in new_posts]
    # Vectorize the preprocessed text
    new_posts_vec = vect.transform(new_posts_processed)
    # Predict the subreddits using the model
    predictions = lr_model.predict(new_posts_vec)
    return predictions


predictions = predict_subreddit(new_data)
for post, prediction in zip(new_data, predictions):
    print(f"Post: {post}\nPredicted subreddit: {prediction}\n")


df_new = pd.DataFrame(new_data, columns=["text"])

# Step 2: Predict subreddits using the previously defined predict_subreddit function
predictions = predict_subreddit(new_data)

# Step 3: Add the predictions as a new column
df_new["predicted_subreddit"] = predictions

# Step 4: Display the resulting DataFrame
df_new.head()



def predictd_subreddit(new_post):
    new_post_vec = vect.transform([new_post])
    predicted_subreddit = lr_model.predict(new_post_vec)
    return predicted_subreddit


prediction = predictd_subreddit(new_data)
for post, prediction in zip(new_data, predictions):
    print(f"Post: {post}\nPredicted subreddit: {prediction}\n")


def predictd(new_post):
    # Preprocess the input text
    new_post_processed = preprocess_text(new_post)
    
    # Transform the preprocessed text into the vectorized form
    new_post_vec = vect.transform([new_post_processed])
    
    # Predict the subreddit
    return lr_model.predict(new_post_vec)


predictd_po = predictd(new_post)
print(predictd_po)


pri = predictd(new_data)
print(pri)


new = "What couch should I use?"


pred = predictd(new)
print(pred)


cat = 'best region to use'
predin = predictd(cat)
print(predin)


pup = 'best for furry friends'
pre = predictd(pup)
print(pre)






