











#Imports
import praw
import os
import pandas as pd
import requests

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

from bs4 import BeautifulSoup

#Needed to read in the varibles that are sensitive.
from dotenv import load_dotenv
load_dotenv()


#Initialize PRAW and Authenticate API
reddit = praw.Reddit(
    client_id = os.getenv('client_id'),
    client_secret = os.getenv('client_secret'),
    user_agent = 'text_classifier:v1.0 (by DvisionaryS)',
    username = 'DvisionaryS',
    password = os.getenv('password')
)


# Assign a variable to the subreddits of your choosing
subreddit_1 = reddit.subreddit('dogs')
subreddit_2 = reddit.subreddit('personalfinance')


# Function to find 1000 post for the dataset
def fetch_posts(subreddit, n=1000):
    posts = []
    for post in subreddit.new(limit=n):
        posts.append({
            'subreddit': post.subreddit,
            'title': post.title,
            'selftext': post.selftext,
            'utc': post.created_utc
            
        })
    return pd.DataFrame(posts)


#Fetch the post from each subreddits
sub_1 = fetch_posts(subreddit_1)
sub_2 = fetch_posts(subreddit_2)


# Forth fetch to combine with 1st taking out the ones that were repeats
sub_1.head()


sub_1.shape


sub_1['utc'].value_counts()


sub_2.head()


sub_2.shape


sub_2['utc'].value_counts()


sub_2 = sub_2.drop_duplicates(subset=['utc'])


sub_2['utc'].value_counts()


sub_1.to_csv('data/subreddit_1_dogposts3.csv', index=False)
sub_2.to_csv('data/subreddit_2_financeposts3.csv', index=False)





#Third fetch to combine with 1st taking out the ones that were repeats
sub_1.head()


sub_1.shape


sub_1['utc'].value_counts()


sub_2.head()


sub_2.shape


sub_2['utc'].value_counts()


sub_2 = sub_2.drop_duplicates(subset=['utc'])


sub_2['utc'].value_counts()


sub_1.to_csv('data/subreddit_1_dogposts2.csv', index=False)
sub_2.to_csv('data/subreddit_2_financeposts2.csv', index=False)








#Second fetch to combine with 1st taking out the ones that were repeats
sub_1.head()


sub_1.shape


sub_1['utc'].value_counts()


sub_2.head()


sub_2.shape


sub_2['utc'].value_counts()


sub_2 = sub_2.drop_duplicates(subset=['utc'])


sub_2['utc'].value_counts()


sub_1.to_csv('data/subreddit_1_dogposts1.csv', index=False)
sub_2.to_csv('data/subreddit_2_financeposts1.csv', index=False)











#View the data
#Saved this data as a csv with my first pulls
sub_1.head()


#View the amount of data
sub_1.shape


#Check for duplicates
sub_1['utc'].value_counts()


#View the data
sub_2.head()


#View the amount of data
sub_2.shape


#Check for duplicates
sub_2['utc'].value_counts()


#Duplicates have been Identifed so drop them.
#Add to notes why we dropped the dupplicates. [After cleaning remove this]
sub_2 = sub_2.drop_duplicates(subset=['utc'])


sub_2['utc'].value_counts()


#Save the dataset
sub_1.to_csv('data/subreddit_1_dogposts.csv', index=False)
sub_2.to_csv('data/subreddit_2_financeposts.csv', index=False)





df1 = pd.read_csv('./Data/subreddit_1_dogposts.csv')

df_1 = pd.read_csv('./Data/subreddit_1_dogposts1.csv')

df__1 = pd.read_csv ('./Data/subreddit_1_dogposts2.csv')

df1_ = pd.read_csv ('./Data/subreddit_1_dogposts3.csv')

df2 = pd.read_csv('./Data/subreddit_2_financeposts.csv')

df_2 = pd.read_csv('./Data/subreddit_2_financeposts1.csv')

df__2 = pd.read_csv('./Data/subreddit_2_financeposts2.csv')

df2_ = pd.read_csv('./Data/subreddit_2_financeposts3.csv')


reddit_df = pd.concat([df1, df_1, df__1, df1_, df2, df_2, df__2, df2_], axis=0, ignore_index=True)


reddit_df.to_csv('./Data/reddit_combined_df.csv', index=False)


reddit_df.head()


reddit_df['subreddit'] == 'personalfinance'


reddit_df['utc'].value_counts()


red_df = reddit_df.drop_duplicates(subset=['utc'])


red_df['utc'].value_counts()


red_df.to_csv('data/final_reddit_data.csv', index=False)



