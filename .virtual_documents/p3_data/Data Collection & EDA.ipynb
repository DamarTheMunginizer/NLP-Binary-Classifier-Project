





df1 = pd.read_csv('../p3_data/research.csv')


#Imports
import praw
import os
import pandas as pd
import requests
import time
import numpy as np


from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

from bs4 import BeautifulSoup

#Needed to read in the varibles that are sensitive.
from dotenv import load_dotenv
load_dotenv()


#Initialize PRAW and Authenticate API
reddit = praw.Reddit(
    client_id = os.getenv('client_id'),
    client_secret = os.getenv('client_secret'),
    user_agent = 'text_classifier:v1.0 (by DvisionaryS)',
    username = 'DvisionaryS',
    password = os.getenv('password')
)


# Assign a variable to the subreddits of your choosing
subreddit_1 = 'dogs'
subreddit_2 = 'personalfinance'


# Function to find 1000 post for the dataset
def fetch_posts(subreddit_name, total_posts=5000, batch_size=1000):
    subreddit = reddit.subreddit(subreddit_name)  # Access the subreddit
    posts = []
    after = None  # Placeholder for pagination
    fetched_post_ids = set()  # Set to track already fetched post IDs
    loops = total_posts // batch_size  # Number of loops required, adjusted to total posts

    print(f"Fetching posts from subreddit: {subreddit_name}")

    for i in range(loops):
        if i % 10 == 0: 
            print(f"Starting loop {i + 1}/{loops}")  # Print progress every 10 loops

        try:
            # Fetch posts in batches
            batch_posts = subreddit.new(limit=batch_size)
            for post in batch_posts:
                if post is not None:
                    # Skip posts that have already been fetched
                    if post.id not in fetched_post_ids:
                        posts.append({
                            'subreddit': post.subreddit.display_name,  # Export the titles I am interested in
                            'title': post.title,
                            'selftext': post.selftext,
                            'utc': post.created_utc
                        })
                        fetched_post_ids.add(post.id)  # Mark this post as fetched

            # Simulate delay to avoid rate limits
            time.sleep(60)

        except Exception as e:
            print(f"Error occurred: {e}")
            break

    return pd.DataFrame(posts)


#Fetch the post from each subreddits
df_dogs = fetch_posts(subreddit_1)
df_finance = fetch_posts(subreddit_2)


df_dogs.head()


df_finance.head()


df = pd.read_csv('./Data/reddit_combined_df.csv')


df.head()


all_posts = pd.concat([df_dogs, df_finance], ignore_index=True)


all_posts.head()


all_posts.shape


total = pd.concat([all_posts, df], ignore_index=True)


total.shape


duplicates = total[total.duplicated(subset=['utc'])]
duplicates.shape


df_no_duplicates = total.drop_duplicates(subset=['utc'])


df_no_duplicates.head()


df1 = df_no_duplicates


df1.shape


df1.to_csv('research.csv',index=False)


df1.info()


df = df1[['title','selftext','subreddit','utc']]


df.head()


#Check for null values
df.isnull().sum()


#Check for blank strings
def blanks(df):
    print((df[df.columns] == '').sum())

blanks(df)


#Fill with nas so we can drop the blanks as there is only 9
df['selftext'].replace('', np.nan, inplace=True)
df.dropna(inplace=True)
blanks(df)


subreddit_counts=df['subreddit'].value_counts()


#Get the lowest subreddit count and downsample so they are even
min_count = min(subreddit_counts)
df_balanced = df.groupby('subreddit').apply(lambda x: x.sample(min_count, random_state=42))


#Reset index after balancing
df_balanced = df_balanced.reset_index(drop=True)


#dropped the excess so that the data is even for modeling accuracy
df_balanced['subreddit'].value_counts()


df_balanced.to_csv('project_cdf.csv',index=False)


from sklearn.utils import shuffle
df_balanced = shuffle(df_balanced)





df_balanced['subreddit'] = df_balanced['subreddit'].map(dict(dogs=1, personalfinance=0))


df_balanced['subreddit'].value_counts()


df_balanced.head()


df_balanced.to_csv('final_df.csv',index=False)





#View the data
#Saved this data as a csv with my first pulls
sub_1.head()


#View the amount of data
sub_1.shape


#Check for duplicates
sub_1['utc'].value_counts()


#View the data
sub_2.head()


#View the amount of data
sub_2.shape


#Check for duplicates
sub_2['utc'].value_counts()


#Duplicates have been Identifed so drop them.
#Add to notes why we dropped the dupplicates. [After cleaning remove this]
sub_2 = sub_2.drop_duplicates(subset=['utc'])


sub_2['utc'].value_counts()


#Save the dataset
sub_1.to_csv('data/subreddit_1_dogposts.csv', index=False)
sub_2.to_csv('data/subreddit_2_financeposts.csv', index=False)





df1 = pd.read_csv('./Data/subreddit_1_dogposts.csv')

df_1 = pd.read_csv('./Data/subreddit_1_dogposts1.csv')

df__1 = pd.read_csv ('./Data/subreddit_1_dogposts2.csv')

df1_ = pd.read_csv ('./Data/subreddit_1_dogposts3.csv')

df2 = pd.read_csv('./Data/subreddit_2_financeposts.csv')

df_2 = pd.read_csv('./Data/subreddit_2_financeposts1.csv')

df__2 = pd.read_csv('./Data/subreddit_2_financeposts2.csv')

df2_ = pd.read_csv('./Data/subreddit_2_financeposts3.csv')


reddit_df = pd.concat([df1, df_1, df__1, df1_, df2, df_2, df__2, df2_], axis=0, ignore_index=True)


reddit_df.to_csv('./Data/reddit_combined_df.csv', index=False)


reddit_df.head()


reddit_df['subreddit'] == 'personalfinance'


reddit_df['utc'].value_counts()


red_df = reddit_df.drop_duplicates(subset=['utc'])


red_df['utc'].value_counts()


red_df.to_csv('data/final_reddit_data.csv', index=False)


sub_2 = sub_2.drop_duplicates(subset=['utc'])


sub_2['utc'].value_counts()


sub_1.to_csv('data/subreddit_1_dogposts1.csv', index=False)
sub_2.to_csv('data/subreddit_2_financeposts1.csv', index=False)


sub_1.shape


sub_1['utc'].value_counts()


sub_2.head()


sub_2.shape


sub_2['utc'].value_counts()


sub_2['utc'].value_counts()


sub_2 = sub_2.drop_duplicates(subset=['utc'])


sub_2['utc'].value_counts()


sub_1.to_csv('data/subreddit_1_dogposts2.csv', index=False)
sub_2.to_csv('data/subreddit_2_financeposts2.csv', index=False)








#Second fetch to combine with 1st taking out the ones that were repeats
sub_1.head()


#Third fetch to combine with 1st taking out the ones that were repeats
sub_1.head()


sub_1.shape


sub_1['utc'].value_counts()


sub_2.head()


sub_2.shape


sub_2.shape


sub_2['utc'].value_counts()


sub_2 = sub_2.drop_duplicates(subset=['utc'])


sub_2['utc'].value_counts()


sub_1.to_csv('data/subreddit_1_dogposts3.csv', index=False)
sub_2.to_csv('data/subreddit_2_financeposts3.csv', index=False)





sub_1.shape


sub_1['utc'].value_counts()


sub_2.head()


# Forth fetch to combine with 1st taking out the ones that were repeats
sub_1.head()
